# Training configuration

# Model config to use
model_config: "configs/model/qwen-7b-lora.yaml"

# Data config to use
data_config: "configs/data/synthesis.yaml"

# Training hyperparameters
training:
  output_dir: "outputs/checkpoints"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  max_grad_norm: 0.3

  # Evaluation
  eval_strategy: "steps"
  eval_steps: 100
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 3

  # Logging
  logging_dir: "outputs/logs"
  logging_steps: 10
  report_to: "wandb"

  # Memory optimization
  gradient_checkpointing: true
  optim: "paged_adamw_32bit"

  # Mixed precision
  bf16: true
  tf32: true

# SFTTrainer specific
sft:
  max_seq_length: 4096
  packing: false
  dataset_text_field: "text"

# Weights & Biases
wandb:
  project: "vicky-trainer"
  name: null  # Auto-generated if null
  tags:
    - "qwen-7b"
    - "lora"
