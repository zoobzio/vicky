# Pre-training configuration for local testing (RTX 3060 Ti)

model_config: "configs/model/qwen-1.5b-lora.yaml"
data_config: "configs/data/pretrain.yaml"

training:
  output_dir: "outputs/checkpoints/pretrain-local"
  num_train_epochs: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  max_grad_norm: 1.0

  eval_strategy: "steps"
  eval_steps: 20
  save_strategy: "steps"
  save_steps: 50
  save_total_limit: 2

  logging_dir: "outputs/logs/pretrain-local"
  logging_steps: 5
  report_to: "none"

  gradient_checkpointing: true
  optim: "paged_adamw_32bit"

  bf16: true
  tf32: true

sft:
  max_seq_length: 1024
  packing: true
  dataset_text_field: "text"
